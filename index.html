<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <title>Visualization is all you need</title>
    <link rel="stylesheet" href="fontawesome-5.5/css/all.min.css" />
    <link rel="stylesheet" href="slick/slick.css">
    <link rel="stylesheet" href="slick/slick-theme.css">
    <link rel="stylesheet" href="magnific-popup/magnific-popup.css">
    <link rel="stylesheet" href="css/bootstrap.min.css" />
    <link rel="stylesheet" href="css/tooplate-infinite-loop.css" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!--
Tooplate 2117 Infinite Loop
https://www.tooplate.com/view/2117-infinite-loop
-->

  </head>
  <body>    
    <!-- Hero section -->
    <section id="XAI" class="text-white tm-font-big tm-parallax">
      <!-- Navigation -->
      <nav class="navbar navbar-expand-md tm-navbar" id="tmNav">              
        <div class="container">   
          <div class="tm-next">
              <a href="#XAI" class="navbar-brand">Explainable AI</a>
          </div>             
            
          <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fas fa-bars navbar-toggler-icon"></i>
          </button>
          <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <ul class="navbar-nav ml-auto">
              <li class="nav-item">
                  <a class="nav-link tm-nav-link" href="#XAI">Home</a> <!-- infinity-->
              </li>
              <li class="nav-item">
                  <a class="nav-link tm-nav-link" href="#introduction">Introduction</a>
              </li>
              <li class="nav-item">
                <a class="nav-link tm-nav-link" href="#basics">Basics</a>
              </li>
              <li class="nav-item">
                <a class="nav-link tm-nav-link" href="#architecture">Architecture</a>
              </li>
              <li class="nav-item">
                  <a class="nav-link tm-nav-link" href="#attention">Attention</a>
              </li>
              <li class="nav-item">
                <a class="nav-link tm-nav-link" href="#train">Training</a> 
            </li> 
              <li class="nav-item">
                  <a class="nav-link tm-nav-link" href="#tips">Details</a> <!--contact-->
              </li> 
              <li class="nav-item">
                <a class="nav-link tm-nav-link" href="#more">More</a>
            </li>                     
            </ul>
          </div>        
        </div>
      </nav>
      
      <div class="text-center tm-hero-text-container">
        <div class="tm-hero-text-container-inner">
            <h2 class="tm-hero-title"> <b> Visualization is all you need</b></h2>
            <p class="tm-hero-subsubtitle">Transformers XAI - Understand the basics of the state-of-the-art </p>
        </div>        
      </div>

 
    </section>

    <section id="introduction" class="tm-section-pad-top">
      
      <div class="container">

            <div class="row tm-content-box"><!-- first row -->
              <div class="col-lg-12 col-xl-12">
                <div class="tm-intro-text-container">
                  <h2 class="tm-text-primary mb-4 tm-section-title">Transformers?</h2>
                  <div class="row">
                                            
                    <div class="col-sm-12 col-md-6">
                      <figure class="tm-transformer-item">
                        <img src="img/transformers.png" alt="Image" class="img-fluid mx-auto" aling=right width=1000000><p class="mb-4 tm-intro-text">
                      </figure>
                      
                    </div>
                    
                    <div class="col-sm-26 col-md-6">
                      <p class="mb-4 tm-intro-text">
                      If you ask what is a Transformer, in general, people will easily define a car that becomes a killer machine (or something like this).
                      However, in the NLP field a Transformer is not even close to any car, or any killer, nor anything like this. <br>
                      In 2017, Google introduced to the world <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener noreferrer">Attention is all you need</a>, the paper that would not only change the meaning of what a Transformer is but also revolutionize 
                      and improve the machine translation results until the moment.   
                      </p>                     
                    </div>
                    
                    
                  </div>
                </div>
              </div>
            </div>

            <div class="row tm-content-box"><!-- first row -->
              <div class="col-lg-12 col-xl-12">
                <h2 class="tm-text-primary mb-4 tm-section-title">Why?</h2>
              </div>
            </div><!-- first row -->
            
            <div class="row tm-content-box"><!-- second row -->
              <div class="col-lg-1">
                      <i class="fas fa-3x fa-trophy text-center tm-icon"></i>
                      <!--<i class="fas fa-3x fa-chart-pie text-center tm-icon"></i>-->
                  </div>
                  <div class="col-lg-5">
                      <div class="tm-intro-text-container">
                          <h2 class="tm-text-primary mb-4">State-of-the-art</h2>
                          <p class="mb-4 tm-intro-text">
                            The paper achieved a never-seen-before results in the field, improving all the previous performances previously reached.
                            It combined some of the best approaches until then: a seq-to-seq model, attention mechanisms and Encoder-Decoder architectures.</p>
                      </div>
                  </div>
                  
                  <div class="col-lg-1">
                      <i class="fas fa-3x fa-thumbtack text-center tm-icon"></i> 
                      <!-- <i class="fas fa-chart-pie"></i> -->
                  </div>
                  <div class="col-lg-5">
                      <div class="tm-intro-text-container">
                          <h2 class="tm-text-primary mb-4">Simplicity</h2>
                          <p class="mb-4 tm-intro-text">
                            Up until that moment, the best approaches used a great variety of complex methods and architectures to achieve 
                            good results. Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs) such as LSTMs, etc.</p>
                      </div>
                  </div>
  
              </div><!-- second row -->

            <div class="row tm-content-box"><!-- second row -->
        		<div class="col-lg-1">
                    <i class="fas fa-3x fa-exclamation-triangle text-center tm-icon"></i>
                    <!--<i class="fas fa-3x fa-chart-pie text-center tm-icon"></i>-->
                </div>
                <div class="col-lg-5">
                    <div class="tm-intro-text-container">
                        <h2 class="tm-text-primary mb-4">Attention</h2>
                        <p class="mb-4 tm-intro-text">
                          The attention mechanism allows the NN to understand the sequence of words in a way similar to what we humans do, 
                          permitting distinction between relevant and irrelevant words inside of it.</p>
                    </div>
                </div>
                
                <div class="col-lg-1">
                    <i class="fas fa-3x fa-stopwatch text-center tm-icon"></i> 
                    <!-- <i class="fas fa-chart-pie"></i> -->
                </div>
                <div class="col-lg-5">
                    <div class="tm-intro-text-container">
                        <h2 class="tm-text-primary mb-4">Velocity</h2>
                        <p class="mb-4 tm-intro-text">
                          The Encoder-Decoder architecture alongside with the multi-head attention implementation, 
                          allows enough parallelization to have a quality model in 12h with P100 GPUs.</p>
                    </div>
                </div>

            </div><!-- second row -->
            
            <div class="row tm-content-box"><!-- third row -->
        		<div class="col-lg-1">
                    <i class="fas fa-3x fa-puzzle-piece text-center tm-icon"></i>
                </div>
                <div class="col-lg-5">
                    <div class="tm-intro-text-container">
                        <h2 class="tm-text-primary mb-4">Adaptability</h2>
                        <p class="mb-4 tm-intro-text">
                      The structure of the data and training methodology makes it suitable for different tasks like machine translation, 
                      language modelling, feature extraction... And allows the possibility of easily adopt the weights to fine-tune for your own purposes. </p>
                    </div>
                </div>
                
                <div class="col-lg-1">
                    <i class="far fa-3x fa-eye text-center tm-icon"></i>
                </div>
                <div class="col-lg-5">
                    <div class="tm-intro-text-container">
                      <h2 class="tm-text-primary mb-4">An eye on the past</h2>
                        <p class="mb-4 tm-intro-text">
                      The attention mechanism, the positional encoding and the skip connections allow 
                      to know where in the sentence we are and not forget to what we have seen without overflowing 
                      the model or leading to exploding gradients due to long sentences.</p>
                          <!--
                          <div class="tm-continue">
                            <a href="#architecture" class="tm-intro-text tm-btn-primary">Details</a>
                        </div> -->
                    </div>
                </div>

            </div><!-- third row -->
            
            <div class="row tm-content-box"><!-- first row -->
              <div class="col-lg-12 col-xl-12">
                <div class="tm-intro-text-container">
                  <h2 class="tm-text-primary mb-4 tm-section-title">How? </h2>                
                    <p class="mb-4 tm-intro-text">
                     All in all, Transformers are a very powerful tool to develop Deep Learning projects. Despite of it, it is essential to understand the background and have a good 
                     knowledge on how this works and runs internally. <br>
                     In this project we approach to Explainable AI in order to comprehend the basis of the Transformers, so anyone who is interested on the theme can expand their notions on the theme 
                     via visualization and interaction. <br> <br>
                     All the results and visualizations shown have been extracted using a real transformer and the task we will focus on is Translation, concretely from English to French. Nonetheless, 
                     Transformers are also suitable to perform <strong>language modeling</strong>. <br>
                    The network used is pretrained with the  <a href="http://www.statmt.org/wmt14/translation-task.html" target="_blank" rel="noopener noreferrer">ACL 2014 ninth workshop
                     on statistical machine translation dataset</a> and no fine-tuning has been applied. The details on the network and the implementation are described in <a href="https://arxiv.org/abs/1806.00187" target="_blank" rel="noopener noreferrer">Scaling Neural Machine Translation</a> 
                     and the source code is available in the <a href="https://github.com/pytorch/fairseq" target="_blank" rel="noopener noreferrer">transformer.wmt14.en-fr fairseq Github repository</a>.                

                    </p>
                  </div><!-- row ending -->
                </div>
              </div>
            </div>

        </div>
      
    </section>

    <section id="basics" class="tm-section-pad-top">
      
      <div class="container">
          <div class="row tm-content-box"><!-- title -->
              <div class="col-lg-12 col-xl-12">
                <h2 class="text-white mb-4 tm-section-title">Basic Notions</h2>
                <p class="mb-4 tm-basics-text">
                  Before we begin, we will clarify some basic notions that will help to understand how the Transformer works and how does it achieve the results it does. To do so, we will
                  cover up the most important mechanisms it applies to interpret the natural language the way humans do. All along, we will follow the steps by using analogies with the sentence
                  <strong>My mum eats a carrot</strong>.
                </p>
              </div>
            </div><!-- title -->

          <div class="row tm-content-box"><!-- first row -->
                <div class="col-sm-6">
                  <div class="tm-basics-text-container">
                    <img src="diagrams/seq2seq.png" style="width:75%" class="arch-center">
                    <div class="caption">Seq-to-One vs. Seq-to-Seq</div>
                  </div>
                </div>

                <div class="col-sm-6">
                  <div class="tm-basics-text-container">
                      <h2 class="tm-text-primary-basics mb-4">Seq-to-Seq</h2>
                      <p class="mb-4 tm-basics-text">
                        Seq-to-Seq (also known as Seq2Seq) is the abbreviation of sequence-to-sequence, which is the characteristic that indicates that a model is trained with sequences of data (rather than 
                        single units) and transforms them into another sequence in some other domain. In a machine translation context, this means that the algorithm works with a sequence of language units from the source language (English in our case) and 
                        translates the whole sequence into the target language (French).
                  </div>
              </div>

          </div><!-- first row -->
            
            <div class="row tm-content-box"><!-- first row -->

                <div class="col-sm-6">
                    <div class="tm-basics-text-container">
                        <h2 class="tm-text-primary-basics mb-4">Embeddings</h2>
                        <p class="mb-4 tm-basics-text">
                          An embedding is the representation of some unit of the natural language in the form of a vector of real numbers. <br>
                          For instance, an embedding could be a vector of 26 numbers where each of them represented the i-th letter of the alphabet and the quantity of the times such letter was found in a certain word.
                          However, embeddings tend to be more complex than this last example in which <strong>coin</strong> and <strong> icon </strong> would provide the same representation and that would be problematic.
                          Recall that the vector represents the structure of the language we choose, not necessarily words: they could represent a letter, part of a word, a word or a whole sentence itself.
                          Although, the most common is for them to be a word or part of it.</p>
                    </div>
                </div>
                
                <div class="col-sm-6">
                  <p class="mb-4 tm-basics-text-plot"> \(\to\) See the difference between the embeddings of the words of different sentences and the subtraction of the sentences' embeddings</p>
                    <div class="tm-basics-text-container">
                      <object width="2000" height="360" data="plots/embeddings.html"> </object>
                      <div class="caption">First 150 values of real embeddings of dimension 1024</div>
                    </div>
                </div>

            </div><!-- first row -->
            
            
            <div class="row tm-content-box"><!-- second row -->

                <div class="col-sm-6">
                  <p class="mb-4 tm-basics-text-plot"> \(\to\) Compare the differences between words and tokens with the same examples</p>
                  <div class="tm-basics-text-container">
                    <object width="1550" height="360" data="plots/tokens.html"> </object>
                    <div class="caption">First 150 values of real embeddings of dimension 1024</div>
                  </div>
                </div>
                
                <div class="col-sm-6">
                    <div class="tm-basics-text-container">
                      <h2 class="tm-text-primary-basics mb-4">Tokens</h2>
                      <p class="mb-4 tm-basics-text">
                        In this Transformer architecture, each embedding represents a part of the word. The tokenization found depends on several aspects:
                        the algorithm of tokenization chosen, the frequency it has on the language and the encoding that is applied. <br>
                        In addition, in this case, punctuation signs count with an own token (commas, apostrophes, etc) and a special one (&#60eos&#62) is used to indicate end of sentence. However, we 
                        insist that this is characteristic of the chosen architecture and, thus, not a generalized or standard processing of the words.<br>
                        One rapidly sees that the relationship between tokens and words is not linear, as it can be appreciated on the example.
                    </div>
                </div>

            </div> <!-- second row -->

            <div class="row tm-content-box"><!-- third row -->              
              <div class="col-sm-6">
                  <div class="tm-basics-text-container">
                    <h2 class="tm-text-primary-basics mb-4">Attention</h2>
                    <p class="mb-4 tm-basics-text">
                    The attention mechanism permits the network to identify which tokens are more related between them. In human terms, it is the 
                  intuition that tells you that what is eaten is the carrot and who eats it is my mum. Mathematically, this is more abstract and harder to explain but 
                the base of all is the scaled dot-product attention and three specific vectors K, Q and V (shown on the right). We will dig into detail in <a href="#attention">Attention</a>.<br>
                We will also see that the Transformer uses what is called  <strong>multi-head attention</strong> and its types.
              </p>
                  </div>
              </div>
              <div class="col-sm-6">
                <div class="tm-basics-text-container">
                  <img src="diagrams/attention.png" style="width:50%" class="arch-center">
                  <div class="caption">Scaled dot-product attention</div>
                </div>
            </div>

          </div> <!-- third row -->

          <div class="row tm-content-box"><!-- fourth row -->

            <div class="col-sm-6">
                <div class="tm-basics-text-container">
                  <br>
                  <object width="1056" height="400" data="plots/sinus_interactive.html"></object>
                  <div class="caption">Sinusoidal functions are usually used for positional encoding</div>
                </div>
            </div>
            
            <div class="col-sm-6">
                <div class="tm-basics-text-container">
                  <h2 class="tm-text-primary-basics mb-4"> Positional encoding </h2>
                  <p class="mb-4 tm-basics-text">
                    We have seen how the Neural Network identifies a word and how it relates their meanings, but how does it determine the order of the words? 
                    Because obviously, saying 'My mum eats a carrot' is not the same as 'A carrot eats my mum'. <br>
                    This is achieved thanks to what we call <strong>positional encoding</strong>, a vector of the same dimension as the embeddings that somehow contains information about the position 
                    of the token within a sentence. Said vectors can be trained weights (optimized during training) or pre-defined functions that work well enough. In the transformer,
                    the autors tried both methods using a mix of sinusoidal functions for the latter one, which they kept, as the results were pretty similar with both techniques.
                    <!--A thorough explanation is found in <a href="#architecture">Architecture</a> and <a href="#tips">Details</a>.</p>-->
                </div>
            </div>
          </div>

          <div class="row tm-content-box"><!-- third row -->              
            <div class="col-sm-6">
                <div class="tm-basics-text-container">
                  <h2 class="tm-text-primary-basics mb-4">Residual connections</h2>
                  <p class="mb-4 tm-basics-text">
                  Also known as skip connections or skip-layer connections, are those steps in which a vector is copied and ommits one ore more layers and then it is used at the output of a posterior layer along with the output itself. There, it can be concatenated or added.  <br>
                  They are useful to avoid vanishing gradient problems and they act in such a way that the network does not 'forget' the past as easily, allowing the model to perform better. 
                  As we will see in the architecture, the Transformer counts with several of them and are important to achieve the results it has. For instance,
                  positional information does not get lost or vanished due to the residual connections.</p>
                </div>
            </div>
            <div class="col-sm-6">
              <div class="tm-basics-text-container">
                <img src="diagrams/skip-con.png" style="width:70%" class="arch-center">
                <div class="caption">Diagram of skip connections</div>
              </div>
          </div>

          </div> <!-- third row -->

        </div> <!-- fourth row -->
        </div>
      
    </section>
    
    <section id="architecture" class="tm-section-pad-top-arch tm-parallax-2">      
      <div class="container tm-architecture-content">
        <div class="row">
          <div class="col-lg-12 tm-content-box">
            <h2 class="text-blackish; text-center mb-4 tm-section-title-architecture">Architecture</h2>
        </div>
      </div>
      <div class="tm-bg-overlay"></div>
    
    <!-- slideshow reference: https://www.w3schools.com/howto/howto_js_slideshow.asp-->
    <!-- Slideshow container -->
    <div class="slideshow-container">

      <!-- Full-width images with number and caption text -->
      <div class="mySlides fade">
        <div class="row tm-content-box">

          <div class="column">
              <div class="tm-basics-text-container">
                  <img src="diagrams/encoder_decoder.png" style="width:100%" class="arch-center">
              </div>
          </div>
          
          <div class="column">
              <div class="tm-basics-text-container">
                <h2 class="tm-text-primary-arch mb-4">Encoder-Decoder</h2>
                  <p class="mb-4 tm-arch-text">
                    This type of architecture is a Neural Network that transforms the data using two modules: the Encoder and the Decoder. <br>
                    The first one transforms the original data into an internal representation of it, that is sometimes referred as <strong>latent data</strong> in the literature.
                    The second one takes said representations and transforms it into a desired target. <br>
                    As for translation purposes, the Encoder works with the source language and the decoder outputs the translation in the target language.
                    If the latent representations were generic enough, just by changing the Decoder one could train a Decoder that translated to any possible language.
                  </p>
              </div>
          </div>
        </div>
      </div>

      <div class="mySlides fade">
        <div class="row tm-content-box">

          <div class="column">
              <div class="tm-basics-text-container">
                  <img src="diagrams/input.png" style="width:75%" class="arch-center">
              </div>
          </div>
          
          <div class="column">
              <div class="tm-basics-text-container">
                <h2 class="tm-text-primary-arch mb-4">Input set up</h2>
                  <p class="mb-4 tm-arch-text">
                    The input received in both parts, the Encoder and the Decoder, is an embedding that represents a token (recall, a part of a word).
                    As this type of Neural Network is not recurrent (a type of network that does have a notion of temporality), it is added to each embedding
                    some specific weights that will determine the position each token has within the sentence, the <strong>positional encoding</strong>. <br>
                    To exemplify, we input the English embeddings of the sentence in the Encoder. In the Decoder, we input the translated right shifted embedding of the encoder input. On other words: the
                    last embedding we predicted in French. <br>
                    To summarise, for each sentence we generate the tokens, we add the positional information and, then, they are ready to enter to the Encoder or the Decoder.
                  </p>
          </div>
        </div>
        </div>
        </div>

      <div class="mySlides fade">
        <h2 class="tm-text-primary-arch mb-4">Positional Encoding</h2>
        <div class="tm-basics-text-container">
          <div class="row">
            <div class="column">
              The weights of the positional encoding can be trained or determined by a function, typically a sinusoidal one. In the paper, said function is the following: <br>
              <br>
              <p style="text-align:center;"> 
                \(PE_{(pos,2i)}=sin(pos/10000^{2i/dmodel})\) <br>
                \(PE_{(pos,2i+1)}=cos(pos/10000^{2i/dmodel})\)
              </p>
              where \(pos\) is the position of the token in the sentence, \(d_{model}\) the dimension of the embeddings and \(i\) represents the position within the embedding.
              <br>
              In other words, in this particular case, to each embedding we add weights that come determined by the alternation of a sine and a cosine function (observable at your right) whose 
              objective is to state in which position a certain token is found.
            </div>
            <div class="column">
              <div class="tm-basics-text-container">
                <img src="plots/pe.png" style="width:95%" class="arch-center">
              </div>
            </div>
          </div>
        </div>
      </div>
      
      <div class="mySlides fade">
        <div class="row tm-content-box">

          <div class="column">
              <div class="tm-basics-text-container">
                  <img src="diagrams/Encoder.png" style="width:75%" class="arch-center">
              </div>
          </div>
          
          <div class="column">
              <div class="tm-basics-text-container">
                <h2 class="tm-text-primary-arch mb-4">Encoder</h2>
                  <p class="mb-4 tm-arch-text">
                    After having the input embeddings prepared, the source language embeddings enter one by one to the Encoder, which will transform them into a
                    latent representation and will eventually go inside the Decoder. <br>
                    It is worth mentioning that, unlike Recurrent Neural Networks, the input is just one vector and therefore we don't have the positional information 
                    contained in any way, this is why positional encoding is needed. <br>
                    Both modules contain mainly two blocks of layers: Feed Forward and attention blocks. In addition, they also count with residual connections and normalization steps.
                    All of them are repeated in a total of N stacks (being N=6). That is, we have 6 times what we see in the diagrams. <br>
                    This module is the most basic of the two.
                  </p>
              </div>
          </div>
        </div>
      </div>

      <div class="mySlides fade">
        <div class="row tm-content-box">
          
            <h2 class="tm-text-primary-arch mb-4">Feed forward blocks</h2>
              <div class="tm-basics-text-container">
                As we mentioned, in each module of the architecture, there can be found two main blocks, the first one is the Feed Forward. They are fully-connected Feed Forward Networks that 
                are applied to each position separately and identically. They consist of two layers with a ReLU activation in between and
                the operations equate the behaviour of two convolutions of a kernel of size 1. <br>
                <br>
                <p style="text-align:center;"> 
                  \(FFN(x) = max(0,xW_1+b_1)W_2+b_2\)
                </p>
                After each stack of layers, both in the Encoder and the Decoder, we have a skip connection and a normalization layer. This 
                helps the network to 'remember' the past and have small values in the embeddings.
              </div>
            <img src="diagrams/feed_forward.png" style="width:90%" class="arch-center">
        </div>
      </div>

      <div class="mySlides fade">
        <div class="row tm-content-box">
          <div class="tm-basics-text-container">
            <h2 class="tm-text-primary-arch mb-4">Attention blocks</h2>
              <img src="diagrams/multihead_attention.png" style="width:70%" class="arch-center">
              <p class="mb-4 tm-arch-text">
                The second main components inside the modules are the Attention blocks. In this paper, it is performed a simple
                attention mechanism (scaled dot-product) that is combined with what it is called <strong>heads</strong>. This combination of techniques result
                in what is known as Multi-Head Attention. <br>
                Each block of attention consists of a linear layer, a scale dot-product attention layer, a concatenation step and a final linear layer.
                Also, one of the blocks (the first found in the Decoder) is referenced as Masked Multi-Head Attention. This is because we don't have all the information when predicting at that step,
                so a mask is applied when training. <br>
                We will provide more detail in <a href="#attention">Attention</a>.
              </p>
          </div>
        </div>
      </div>
      

      <div class="mySlides fade">
        <div class="row tm-content-box">

          <div class="column">
              <div class="tm-basics-text-container">
                  <img src="diagrams/Decoder.png" style="width:60%" class="arch-center">
              </div>
          </div>
          
          <div class="column">
              <div class="tm-basics-text-container">
                <h2 class="tm-text-primary-arch mb-4">Decoder</h2>
                  <p class="mb-4 tm-arch-text">
                    The Decoder architecture is pretty similar to the previous one. In fact, we could say that, after the first Attention block and normalization, the architecture is basically the same. <br>
                    However, the most noticeable differences are the inputs it has. <br>
                    The first (at the beginning of the Decoder) is the target language embeddings shifted right (as we have already mentioned, the last predicted embedding). <br>
                    The second one (in the middle of the module), passes the Encoder output and the Decoder embeddings that we have until that step, as input. <br>
                    Also, in here, we have two Attention blocks, the first one being the masked attention block and the second a regular one. <br>
                    Notice that after the Decoder module we find two final layers, a linear layer and a softmax layer that will be in charge of choosing the most probable translation.
                  </p>
              </div>
          </div>
        </div>
      </div>


      <div class="mySlides fade">
        <h2 class="tm-text-primary-arch mb-4">Full schema</h2>
        <div class="tm-basics-text-container">
          <img src="diagrams/whole.png" style="width:75%" class="arch-center">
        </div>
      </div>

      </div>

      <!-- Next and previous buttons -->
      <a class="prev" onclick="plusSlides(-1)">&#10094;</a>
      <a class="next" onclick="plusSlides(1)">&#10095;</a>
    </div>
    <br>

    <!-- The dots/circles -->
    <div style="text-align:center">
      <span class="dot" onclick="currentSlide(1)"></span>
      <span class="dot" onclick="currentSlide(2)"></span>
      <span class="dot" onclick="currentSlide(3)"></span>
    </div> 

  <!--slider END-->
    </section>
    
    <section id="attention" class="tm-section-pad-top">
      <div class="container tm-container-gallery">
        <div class="row">
          <div class="text-justify col-12">
            <div class="text-white col-12"> 
              <h2 class="tm-text-primary tm-section-title mb-4">Attention</h2>
                <!--<p class="mx-auto tm-section-desc">-->
                
                  <div class="row">
                    <div class="column">
                      <p class="mb-4 tm-intro-text">
                      As we have introduced, attention is in simple words, the mechanism that stablishes the relevance of a token in relationship with another. 
                      <br> To do so, it is necessary to 
                      define the three components used: key (K), query (Q) and values (V). These are the vectors that we combine (map) in such a way that produce an output (also a vector) whose values define the mentioned relationships. <br>
                      The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query (Q) with the corresponding key (K). <br>
                      In other words, K and Q are used to define weights that will be used against V to define the importance between tokens. 
                      We have three blocks in the Transformer architecture that perform this evaluation, but first let's describe better what an attention block does.
                      </p>
                    </div>

                    <div class="column">
                      <img src="diagrams/attention_gray.png" style="width:70%" class="arch-center">
                      <div class="caption">Attention blocks within Transformer architecture</div>
                    </div>
                  </div>
            </div>     
          </div>            
        </div>

        <div class="row">
          <div class="text-justify col-12">
            <div class="text-white col-12">
                  <div class="row">
                    <div class="column">
                        <img src="diagrams/attention_dark.png" style="width:95%" class="arch-center">
                        <div class="caption">Attention block</div>
                    </div>
                    <div class="column">
                      <p class="mb-4 tm-intro-text">
                      The attention itself is calculated as follows:
                      </p>
                        <img src="diagrams/attention_formula.png" style="width:50%" class="arch-center">
                      <p class="mb-4 tm-intro-text">
                        So each of the attention blocks perform this computation using the respective K,Q and V; which come from different sources according to which block we are found. <br>
                        After performing the softmax, the result is scaled by \(d_k\) (the dimension of the embedding) and after it, it is multiplied by V. This way it is ensured that the vectors
                        we deal with are normalized at each step, a fact that is really helpful to avoid overflows and numeric problems.
                      </p>
                    </div>
                  </div>
            </div>     
          </div>       

        
        </div>

        <div class="row">
          <div class="text-justify col-12">
            <div class="text-justify col-12"> 
              <h2 class="tm-text-primary-attention mb-4"> Types </h2>
                  <p class="mb-4 tm-intro-text">               
                    Having all this, one is capable to generate the <strong>attention matrix</strong>. One may recall that the architecure of the Transformers, from which we could
                    extract three attention matrices (as we have three blocks that compute it). <br>
                  </p>
                  <p class="mb-4 tm-intro-text"><b> Self-Attention </b></p>
                    <ul>
                      <li>The first block is found in the Encoder and uses the source language sembeddings as K, Q and V. </li>
                      <li>Therefore, it somehow stablishes relationships among the sentence with itself in the source language, in our case, English.</li>
                    </ul> 

                  <p class="mb-4 tm-intro-text"><b>Masked self-Attention</b></p>
                    <ul>
                      <li>The second block is found in the Decoder. </li>
                      <li>K, Q and V are the input of the decoder which, remember, is the last output predicted (or the shifted right target).</li> 
                      <li>It is called masked because when we train, we mask the future data that we are not able to see when we predict (not having the data masked would imply that the network would see the future, hence it would be "cheating"). </li>
                      <li>In other words, this Attention is computed with what we have so far in French.</li>
                    </ul>

                  <p class="mb-4 tm-intro-text"><b>Encoder-Decoder Attention</b></p>
                    <ul>
                    <li>The third and last block is also found in the Decoder.</li>
                      <li>K and Q are the output of the encoder (therefore, the tokens in the source language, English).</li>
                      <li>V is the information we have of the Decoder up to that point, the shifted right outputs in French.</li>
                      <li>It is called Encoder-Decoder Attention due to the fact that combines the information of the Encoder (source language) and Decoder (target language), but 
                      in terms of computations and structure it has no difference with the previous ones.</li>
                      <li>This might be the most interesting matrix of all three, as it shows the relationships of the words / tokens of the source  and the target language. </li>
                    </ul>
                  
                  <p class="mb-4 tm-intro-text">
                    Here we introduce real examples of the Self-Attention and Encoder-Decoder Attention matrices.
                  </p>
                  </p>
                <div class="text-center col-12">
                  <object width="1056" height="750" data="plots/attention_original.html"></object>
                </div>  
            </div>     
        </div>  
        <div class="row">
          <div class="text-justify col-12">
            <div class="text-justify col-12"> 
              <h2 class="tm-text-primary-attention mb-4"> Heads </h2>
                <p class="mb-4 tm-more-text">
                    Up to this point, you might be wondering what exactly are heads in terms of Attention and what do they have to do in here. <br>
                    Well, conceptually heads are not so willing to explain with an example because they are more like a helping 'feature' rather than a palpable thing itself. <br>
                    Heads is the name assigned to the number of partitions we do to the input vectors (K, Q and V) in order to compute attention separately. This allows the segmentation of the attention
                    calculations and therefore the parallelization of this part of the model. Not only do we focus in smaller parts but also we permit them to be computed in a faster way
                    that allows a faster training. <br>
                    So it is as simple as dividing the input vectors K, Q, and V into <strong>head</strong> parts. Then, they follow the same procedure as previously: a linear layer and the scale dot-product.
                    After it, we just concatenate the 'pieces' and pass through a last linear layer. <br>
                    In the gif it can be observed the case where the number of heads is 3.
                    <div class="text-center col-12">
                      <object style="width:70%" data="diagrams/attention_gif/attention.gif"></object> <br>
                      
                      <source src="diagrams/attention_gif/attention.mp4" type="video/mp4">
                  </div>
            </div>  
          </div>     
        </div>   
      </div>
    </section>

      <!-- Training section -->
      <section id="train" class="tm-section-pad-top">
        <div class="container tm-container-gallery">
          <div class="row">
            <div class="text-white col-12"> 
              <h2 class="text-white mb-4 tm-section-title">Training and usage</h2>
                <!--<p class="mx-auto tm-section-desc">-->
                <p class="text-white mb-4 tm-intro-text">
                  
                It's time to talk a little bit about the software part of the network. We are going to review, in general terms, which is the loss function, how the problem is 
                presented and some details that might be taken into account when building and training such Neural Network.

                  
                <h2 class="tm-text-primary-train mb-4"> Loss function </h2>
                
                <p class="text-white mb-4 tm-intro-text">
                The loss function that is minimized is known as the Cross Entropy Loss and is defined as follows:
                </p>
                
                <p style="text-align:center;"> 
                    \( \displaystyle {L(y,\hat{y}) = - \sum_{k} y^{(k)} log\;( \hat{y}^{(k)} )} \) 
                  </p>
                
                  <p class="text-white mb-4 tm-intro-text">
                Being \(k\) the total number of classes, \(y\) the target probabilities and \(\hat{y}\) the predicted probabilities.
                As can be seen, it is generalized to any number of classes one may desire. It is also vary useful and suitable for 
                classification problems, as it penalizes more the wrong predictions and rewards more the correct one in comparison with what other losses would do.
                </p>
                
                <h2 class="tm-text-primary-train mb-4"> Masks and prediction </h2>
                <p class="text-white mb-4 tm-intro-text">
                We have already talked about masks. A masks is a special token that replaces another token in a way that the Neural Network "does not take it into account". By this we mean that, if a token 
                is replaced by the token &#60mask&#62, the Transformer will have no information on that so it would work as if it wasn't there.
                Masks are used during training in the first Decoder Attention Block as we have seen in the <a href="#architecture">Architecture</a>.
                Similarly, masks are applied in many tasks of NLP, for instance, when training a next-word-prediction network, the future words are also masked so that the network doesn't cheat.
                </p>
                
                
                <h2 class="tm-text-primary-train mb-4"> Do It Yourself</h2>
                <p class="text-white mb-4 tm-intro-text">
                If you wanted to train a Transformer architecture by yourself you would have two options: <br> <br>
              
                Train from scratch with your own data with and your own coded architecture (or one imported from some library). <br> <br>
                This is a tricky one as you would need to take into account: what dimensions to choose, how to perform the data processing (tokenization, deal with case sensitivity),
                choose hyperparameters, optimizers, etc. Let's be honest, coding from scratch is not really common unless you have a special interest on doing all that by yourself. It is more common to take 
                the models available in different libraries and train with your own data, however if you don't have a big amount of data the best option is the next one. <br>
                
                 </p>

                <div class="row">
                  <div class="three_quarter_column">
                    <p class="text-white mb-4 tm-intro-text">
                    You can take a pretrained network and fine-tune.
                    One of the advantages of the Transformers is that they are suitable for many tasks and contexts, typically for NLP, even though they can be used outside the language processing field. 
                    Here we have used the Transformer to translate from English to French. Needless to say, having a proper dataset in any other language would serve to train a translator to said language.
                    Besides, translation is not the only task that this type of architecture can perform. <br> 
                    Due to the outstanding results it provided, many different architectures and varieties arised since the publication of the original paper in 2017 and there are many open-source libraries that provide the code, data and 
                    architecure to be able to train your own network.<br> 
                    </p>
                  </div>

                  <div class="quarter_column">
                    <br>
                    <img src="diagrams/fine_tune.png" style="width:100%" class="arch-center">
                    </p>
                  </div>
                </div>
                
                

                Having all this information, training a Transformer architecture yields a whole world of opportunities:

                </p>
                <!--Here we have used the transformer to translate from English to French. Needless to say, having a proper dataset in any other language would be enough to train a translator to said language.
                    Nonetheless, translation is not the only task that this type of architecture can perform. Among the possibilities we can train transformers to:-->
                    

                <div class="row">
                  <div class="third_column">
                    <i class="fas fa-3x fa-people-carry text-center tm-icon-train"></i> 
                    <h2 class="tm-text-primary-train-center mb-4">Tasks</h2>
                    <ul class='ul'>
                      <li class='li-train'> Next-sentence prediction </li>
                      <li class='li-train'> Question answering </li>
                      <li class='li-train'> Reading comprehension </li>
                      <li class='li-train'> Sentiment analysis </li>
                      <li class='li-train'> Paraphrasing </li>                  
                    </ul>
                  
                  </div>
                  <div class="third_column">
                    <i class="far fa-3x fa-edit text-center tm-icon-train"></i> 
                    <h2 class="tm-text-primary-train-center mb-4">Applications</h2>
                    <ul>
                      <li class='li-train'>Machine translation </li>
                      <li class='li-train'> Document summarization </li>
                      <li class='li-train'> Document generation </li>
                      <li class='li-train'> Named entity recognition </li>
                      <li class='li-train'> Niological sequence analysis</li>
                    </ul>
                    
                  </div>
                  <div class="third_column">
                    <i class="fas fa-3x fa-cogs text-center tm-icon-train"></i> 
                    <h2 class="tm-text-primary-train-center mb-4">Architectures</h2>
                    <ul>
                      <li class='li-train'> GPT-3 </li> 
                      <li class='li-train'> GPT-2 </li>
                      <li class='li-train'> BERT </li>
                      <li class='li-train'> XLNet </li>
                      <li class='li-train'> RoBERTa </li>
                      <li class='li-train'> Distilled BERT</li>
                    </ul>
                    
                  </div>
                </div>
              
            </div>            
          </div> 
        </div>
      </section>

    <!-- Contact -->
    <section id="tips" class="tm-section-pad-top tm-parallax-2">
    
      <!--<div class="container tm-container-contact">-->
        <div class="container tm-container-gallery">
          <div class="row">
            <div class="text-justify col-12">
              <h2 class="text-white mb-4 tm-section-title">Details</h2>
                      <p class="mb-4 tm-basics-text">

                        In this section we provide further detail of some of the concepts already explained, more insight on some structures or elements that are specific for this network or any curiosity or feature that we consider worth mentioning.

                      </p><br>
              
              <!-- collapsible dropdown-->
              <button class="accordion">AutoEncoders</button>
              <div class="panel">
                <div class="row">
                  <div class="column">
                    <img src="diagrams/autoencoder.png" style="width:50%" class="arch-center">
                  </div>
                  <div class="column">
                    <p class="mb-4 tm-basics-text">
                    <br>
                    We have learned what an Encoder-Decoder architecture is, in addition, it is interesting to have a look at the so-called Auto-Encoder. 
                    This type of network is pretty famous and used as it is a simple but effective unsupervised method. <br>
                    It consists of a network that is trained to predict exactly the same as it receives as input (weird right?). The trick of it is in the bottleneck (latent data) that is smaller than the initial and final dimension, 
                    this way we train the autoencoder so that it compresses the most relevant information and remains condensed in a lowered-dimension vector than the original. <br>
                    It is really helpful for compression, image segmentation, face recognition, etc.
                    </p>
                  </div>
                </div>
              </div>
              
              <button class="accordion">Special tokens</button>
              <div class="panel">
                <br>
                <p>Special tokens are also a helpful feature to take into account. We have introduced the end-of-sentence tokens used in the paper. However, it is also usual
                  to find other tokens that do not directly reffer to a word: the beginning of sentence token: &#60bos&#62, 
                  punctuation signs tokens: &#60apos&#62, &#60,&#62, &#60.&#62, etc; and even a special token for unseen words &#60unk&#62.
                  </p>
              </div>
              
              <button class="accordion">BPE</button>
              <div class="panel">
                <br>
                <p>Byte Pair Encoding (or digram coding) is a simple form of data compression. 
                  It consists in replacing the most common pair of consecutive bytes of the data with a byte that does not occur within that data.
                  In NLP, it is a technique that can be applied as part of the preprocessing of the data and can help to reduce, memory-wise, the size
                of the sets (training, validation and testing) considerably. This way, the training is speeded up while the quality is not seen affected. </p>
              </div> 

              <button class="accordion">Label smoothing</button>
              <div class="panel">
                <br>
                <img src="diagrams/label_smooth.png" style="width:85%" class="arch-center">
                <br>
                <p>Label Smoothing is a regularization technique that introduces noise for the labels / target. By doing this we prevent the network to work with absolute zeroes,
                  hence saving it from computational issues. It is an easy, yet really helpful, technique to help the model generalize instead of overfitting by following really
                  strict patterns which, in the end, are not found in real life or when predicting never-seen-before targets.
                  As a matter of fact, it is only used in classification problems that use the cross-entropy function.
                </p>
              </div> 
              
              <button class="accordion">Intuition behind the Positional Encoding</button>
              <div class="panel">
                <br>
                 <p>As we already know, positional encoding is what provides the network with the <strong>order</strong> information within the words or tokens. Quoting the authors:
                  "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, 
                  we must inject some information about the relative or absolute position of the tokens in the sequence." <br></p>
                  
                  <div class="row">
                    <div class="three_quarter_column">
                      <p> However, what do sinusoidal functions have to do with order? <br>
                      It is all related with the way bits change as they grow. If we observe the evolution of the bits when augmenting the numbers from one to five in binary, we observe the behaviour found in the right. <br>
                      Somehow, every time we increase a number, this behaves at some level as a sinusoidal movement, here the reason why this type of function is appropiate for this task. </p>
                    </div>
                    <div class="quarter_column">
                      <img src="diagrams/bits.png" style="width:50%" class="arch-center">
                    </div>
                    <p> Therefore in the paper it is chosen a sine and a cosine function (shown below) that are alternated to simulate the change between numbers in a continuous form.</p>
                  </div>
                  <div class="tm-basics-text-container">
                    <img src="diagrams/sine_cosine.png" style="width:100%" class="arch-center">
                    <div class="caption">Sine and cosine components of PE</div>
                  </div>
              
            </div>

              <button class="accordion">Beam</button>
              <div class="panel">
                <br>
                <p>As this projects intends to understand the overall ideas and intuitions behind the Transformer rather than digging into the details, we have avoided as much as possible 
                  mentioning dimensions and numbers. Furthermore, they have many different factors that can affect them (the type of architecture, the structure of it, the way one may want to train the model,
                  the size of the data, the resources available, etc). However, there is one dimension that is interesting and useful in this type of task and does not always appear explained: the <strong>beam dimension</strong>. <br><br>
                  We could define the beam dimension as the hypothesis dimension we allow the model to investigate. As we already mentioned, the input of the Decoder is the last output and it can drag error.
                  That is why the network takes the top B most probable words to be the translated (being B the beam dimension), predicts each of the B most probable next words / tokens and keeps, from the B first hypothesized words, the one that provides less error. This is repeated at each step.
                  <br> <br>
                  An example, if we want to translate our already known base sentence: <strong>My mum eats a carrot</strong>. We start the process, our beam dimension is \(B=2\), for the first word we hypothesize the translation to be <strong>ma</strong> and <strong>mon</strong>. The network will translate the following word taking into account 
                  both articles: <strong>ma mre</strong> and <strong>mon mre</strong> and will conclude that, based on the error, <strong>ma</strong> is a better prediction.
                  <br><br>
                  This does not magically convert the network in the perfect translation machine, however it does lower the possibility of making a wrong prediction.
                </p>
              </div> 

              <button class="accordion">Softmax function</button>
              <div class="panel">
                <br>
                <p>We have mentioned that after the Decoder, as latest step of the forward pass, we apply the softmax function that is: <br>
                  <p style="text-align:center;"> 
                    \( softmax(x_{i}) = \displaystyle{\frac{\exp(x_i)}{\sum_j \exp(x_j)}}\)
                  </p>
                  The softmax function tries to replicate the behaviour of the maximum function. However, as computing it is not differentiable,
                  it would not be possible for the network to propagate the gradients through it (plainly, we couldn't train the network as it would not update the weights).
                  For this reason a differentiable function that simulates the maximum is good enough; and it is called <strong>soft</strong> as we still preserve a part of the non-maximum labels.
                </p>
              </div> 

              <button class="accordion">Activation functions</button>
              <div class="panel">
                <br>
                <p>Activation functions determine if a neuron will be <strong>activated</strong> according to the value it has received. If this happens, the weights connected to the mentioned neuron will be updated during the backpropagation.
                  
                  In this architecture we have seen the ReLU activation,
                  which is the abbreviation for Rectified Linear Unit. This function activates the neuron in case the input received is higher than 0, that is: </p>
                  <p style="text-align:center;"> 
                    \( ReLU(x) = max(0, x) \)
                  </p>
                <p> However there are lots of different activation functions, each of them more suitable for certain contexts and tasks. Between the most common ones 
                  we can find the tangent activation function, sigmoid, linear... and many combinations or versions of it: Leaky ReLU, ELU, maxout, etc. Furthermore, the softmax function could also be included in this list.
                </p>
              </div> 
 
              <button class="accordion">Convolutions and CNNs</button>
              <div class="panel">
                <br>
                <p>The convolution is a type of operation usually performed in signal processing. It works with two functions: the main signal and 
                  the kernel function and it expresses how the shape of one is modified by the other. Mathematically it is written as:</p>
                  <p style="text-align:center;"> 
                    \( y(t) = x(t) * h(t) = \displaystyle{ (x*h)(t):=\int _{-\infty }^{\infty }x(\tau )h(t-\tau )\,d\tau }\)
                  </p>
                <p> Being \(y(t)\) the convolution result, \(x(t)\) the signal and \(h(t)\) the kernel. <br>
                  It has many uses and applications, even in Deep Learning. Long story short, convolutional Neural Networks (CNNs) use  the signal \(h(t) \)
                  as weights to be trained. Nowadays, they are one of the best networks to use in image and video recognition, image classification, etc. And as we have already 
                  pointed out, the linear layer block weights act as a convolution of kernel size 1.
                </p>
              </div> 

              </div><!-- row ending -->
              
      </div>

    </section>


    <!-- More section -->
    <section id="more" class="tm-section-pad-top">
      <div class="container tm-container-gallery">
        <div class="row">
          <div class="text-white col-12"> 
              <h2 class="tm-text-primary tm-section-title mb-4">Learn more</h2>
              <!--<p class="mx-auto tm-section-desc">-->
              <p class="mb-4 tm-more-text">
                The Artificial Intelligence universe is wide and it will soon be infinite, if you are interested in the topic, here you can find a bunch of resources that cover a variety of related work.
                More state-of-the-art architectures, visualizing tools to understand them and social research upon ethics and bias inside NLP.
              </p>
          </div>            
        </div>
      
        <div class="row_more">
          <div class="column_more"> 
            <div class="box">  
              <img id="image-3" src="img/1.png"/> 
              <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer"class="item-link">  
              <span class="caption fade-caption">  
                <h3>Original Paper</h3> <br>
                <p>Attention is all you need (2017)</p>  
              </span>  
              </a>
            </div>         
          </div>
          <div class="column_more"> 
            <div class="box">  
              <img id="image-3" src="img/2.png"/>  
              <span class="caption fade-caption">  
              <h3>Visualization is all you need</h3>  
              <p>The paper behind this project</p>  
              </span>  
            </div>         
          </div>
          <div class="column_more"> 
            <div class="box">  
              <img id="image-3" src="img/3.png"/>
              <a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener noreferrer" class="item-link">  
              <span class="caption fade-caption">  
                <h3>Bert</h3>  
                <p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)</p>  
              </span>
              </a> 
            </div>         
          </div>
          <div class="column_more"> 
            <div class="box">  
              <img id="image-3" src="img/4.png"/>
              <a href="https://www.youtube.com/watch?v=66TgH0YV4eA" target="_blank" rel="noopener noreferrer" class="item-link">  
                <span class="caption fade-caption">  
                  <h3>Visualization Best Practices for Explainable AI</h3>  
                  <p>Videoconference to know more about XAI</p>  
                </span> 
              </a> 
            </div>         
          </div>
          <div class="column_more"> 
            <div class="box">  
              <img id="image-3" src="img/5.png"/>  
              <a href="https://www.nature.com/articles/s42256-019-0105-5.epdf?shared_access_token=5NdesGgrExhcnuTJRe8DNdRgN0jAjWel9jnR3ZoTv0NEudUkYwGyjqChcABtnX-6eKbIMBVbpkj9zwX9KiOaMQVcC8y1K81Jl_1-07XWNIqFe_Bwm2zoUm06O12CThusfZ938gKciIC-qMvrbtNvOg%3D%3D" target="_blank" rel="noopener noreferrer" class="item-link">  
                <span class="caption fade-caption">  
                  <h3>Gender bias in AI</h3>  
                  <p>An analysis of gender bias studies in natural language processing</p>  
                </span> 
              </a> 
              </a>
            </div>         
          </div>
        </div>
        <div class="row_more">
          <div class="column_more"> 
            <div class="box">  
              <img id="image-3" src="img/6.png"/>  
              <a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="noopener noreferrer" class="item-link">  
                <span class="caption fade-caption">  
                  <h3>ELMo</h3>  
                  <p>Deep contextualized word representations (2018)</p>  
                </span>
              </a> 
            </div>         
          </div>
          <div class="column_more"> 
            <div class="box">  
              <img id="image-3" src="img/7.png"/>  
              <a href=" https://projector.tensorflow.org/" target="_blank" rel="noopener noreferrer" class="item-link">  
                <span class="caption fade-caption">  
                  <h3>Embedding porjector</h3>  
                  <p>Interactive embeddings projections</p>  
                </span>
              </a> 
            </div>         
          </div>
          <div class="column_more"> 
            <div class="box">  
              <img id="image-3" src="img/8.png"/>  
              <a href="https://arxiv.org/abs/2002.03808" target="_blank" rel="noopener noreferrer" class="item-link">  
                <span class="caption fade-caption">  
                  <h3>Transformers in VOICE</h3>  
                  <p>Vocoder-free End-to-End Voice Conversion with Transformer Network (2020)</p>  
                </span>
              </a>  
            </div>         
          </div>
          <div class="column_more"> 
            <div class="box">
              <img id="image-3" src="img/9.png"/>  
              <a href="https://www.washingtonpost.com/opinions/2018/12/17/why-your-ai-might-be-racist/?noredirect=on" target="_blank" rel="noopener noreferrer" class="item-link">  
                <span class="caption fade-caption">  
                  <h3>Racism and AI</h3>  
                  <p>Racist behaviour in NN</p>  
                </span>
              </a>
            </div>         
          </div>
          <div class="column_more"> 
            <div class="box">  
              <img id="image-3" src="img/10.png"/>
              <a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53" target="_blank" rel="noopener noreferrer" class="item-link">  
                <span class="caption fade-caption">  
                  <h3>Convolutional NN</h3>  
                  <p>Article for CNN beginners</p>  
                </span>
              </a>
              </div>         
          </div>
        </div>
      </div>
      </div>

      <footer class="text-center small tm-footer">
        <p class="mb-0">
        Copyright &copy; 2020 Maria Ribalta
        
        . <a rel="nofollow" href="https://www.tooplate.com" target="_blank" rel="noopener noreferrer" title="HTML templates">Inspired by TOOPLATE</a></p>
      </footer>
     
    </section>



    <script src="js/jquery-1.9.1.min.js"></script>     
    <script src="slick/slick.min.js"></script>
    <script src="magnific-popup/jquery.magnific-popup.min.js"></script>
    <script src="js/easing.min.js"></script>
    <script src="js/jquery.singlePageNav.min.js"></script>     
    <script src="js/bootstrap.min.js"></script> 
    <script>

    // TRICKS - BEG OF COLLAPSIBLE
    
    var acc = document.getElementsByClassName("accordion");
    var i;

    for (i = 0; i < acc.length; i++) {
      acc[i].addEventListener("click", function() {
        this.classList.toggle("active-acc");
        var panel = this.nextElementSibling;
        if (panel.style.maxHeight) {
          panel.style.maxHeight = null; //null
        } else {
          panel.style.maxHeight = panel.scrollHeight + "px"; 
        }
      });
    }

    //END OF COLLAPSIBLE

    // BEG OF SLIDER

        var slideIndex = 1;
        showSlides(slideIndex);

        // Next/previous controls
        function plusSlides(n) {
          showSlides(slideIndex += n);
        }

        // Thumbnail image controls
        function currentSlide(n) {
          showSlides(slideIndex = n);
        }

        function showSlides(n) {
          var i;
          var slides = document.getElementsByClassName("mySlides");
          var dots = document.getElementsByClassName("dot");
          if (n > slides.length) {slideIndex = 1}
          if (n < 1) {slideIndex = slides.length}
          for (i = 0; i < slides.length; i++) {
              slides[i].style.display = "none";
          }
          for (i = 0; i < dots.length; i++) {
              dots[i].className = dots[i].className.replace(" active", "");
          }
          slides[slideIndex-1].style.display = "block";
          dots[slideIndex-1].className += " active";
        } 

    // END OF SLIDER

      function getOffSet(){
        var _offset = 450;
        var windowHeight = window.innerHeight;

        if(windowHeight > 500) {
          _offset = 400;
        } 
        if(windowHeight > 680) {
          _offset = 300
        }
        if(windowHeight > 830) {
          _offset = 210;
        }

        return _offset;
      }

      function setParallaxPosition($doc, multiplier, $object){
        var offset = getOffSet();
        var from_top = $doc.scrollTop(),
          bg_css = 'center ' +(multiplier * from_top - offset) + 'px';
        $object.css({"background-position" : bg_css });
      }

      // Parallax function
      // Adapted based on https://codepen.io/roborich/pen/wpAsm        
      var background_image_parallax = function($object, multiplier, forceSet){
        multiplier = typeof multiplier !== 'undefined' ? multiplier : 0.5;
        multiplier = 1 - multiplier;
        var $doc = $(document);
        // $object.css({"background-attatchment" : "fixed"});

        if(forceSet) {
          setParallaxPosition($doc, multiplier, $object);
        } else {
          $(window).scroll(function(){          
            setParallaxPosition($doc, multiplier, $object);
          });
        }
      };

      var background_image_parallax_2 = function($object, multiplier){
        multiplier = typeof multiplier !== 'undefined' ? multiplier : 0.5;
        multiplier = 1 - multiplier;
        var $doc = $(document);
        $object.css({"background-attachment" : "fixed"});
        
        $(window).scroll(function(){
          if($(window).width() > 768) {
            var firstTop = $object.offset().top,
                pos = $(window).scrollTop(),
                yPos = Math.round((multiplier * (firstTop - pos)) - 186);              

            var bg_css = 'center ' + yPos + 'px';

            $object.css({"background-position" : bg_css });
          } else {
            $object.css({"background-position" : "center" });
          }
        });
      };
      
      $(function(){
        // Hero Section - Background Parallax
        background_image_parallax($(".tm-parallax"), 0.30, false);
        background_image_parallax_2($("#tips"), 0.80);   
        background_image_parallax_2($("#architecture"), 0.80);   
        
        // Handle window resize
        window.addEventListener('resize', function(){
          background_image_parallax($(".tm-parallax"), 0.30, true);
        }, true);

        // Detect window scroll and update navbar
        $(window).scroll(function(e){          
          if($(document).scrollTop() > 120) {
            $('.tm-navbar').addClass("scroll");
          } else {
            $('.tm-navbar').removeClass("scroll");
          }
        });
        
        // Close mobile menu after click 
        $('#tmNav a').on('click', function(){
          $('.navbar-collapse').removeClass('show'); 
        })

        // Scroll to corresponding section with animation
        $('#tmNav').singlePageNav({
          'easing': 'easeInOutExpo',
          'speed': 600
        });        
        
        // Add smooth scrolling to all links
        // https://www.w3schools.com/howto/howto_css_smooth_scroll.asp
        $("a").on('click', function(event) {
          if (this.hash !== "") {
            event.preventDefault();
            var hash = this.hash;

            $('html, body').animate({
              scrollTop: $(hash).offset().top
            }, 600, 'easeInOutExpo', function(){
              window.location.hash = hash;
            });
          } // End if
        });

        // Pop up
        $('.tm-gallery').magnificPopup({
          delegate: 'a',
          type: 'image',
          gallery: { enabled: true }
        });

        $('.tm-architecture-carousel').slick({
          dots: true,
          prevArrow: false,
          nextArrow: false,
          infinite: false,
          slidesToShow: 3,
          slidesToScroll: 1,
          responsive: [
            {
              breakpoint: 992,
              settings: {
                slidesToShow: 2
              }
            },
            {
              breakpoint: 768,
              settings: {
                slidesToShow: 2
              }
            }, 
            {
              breakpoint: 480,
              settings: {
                  slidesToShow: 1
              }                 
            }
          ]
        });

        // Gallery
        $('.tm-gallery').slick({
          dots: true,
          infinite: false,
          slidesToShow: 5,
          slidesToScroll: 2,
          responsive: [
          {
            breakpoint: 1199,
            settings: {
              slidesToShow: 4,
              slidesToScroll: 2
            }
          },
          {
            breakpoint: 991,
            settings: {
              slidesToShow: 3,
              slidesToScroll: 2
            }
          },
          {
            breakpoint: 767,
            settings: {
              slidesToShow: 2,
              slidesToScroll: 1
            }
          },
          {
            breakpoint: 480,
            settings: {
              slidesToShow: 1,
              slidesToScroll: 1
            }
          }
        ]
        });
      });

      function includeHTML() {
        var z, i, elmnt, file, xhttp;
        /* Loop through a collection of all HTML elements: */
        z = document.getElementsByTagName("*");
        for (i = 0; i < z.length; i++) {
          elmnt = z[i];
          /*search for elements with a certain atrribute:*/
          file = elmnt.getAttribute("w3-include-html");
          if (file) {
            /* Make an HTTP request using the attribute value as the file name: */
            xhttp = new XMLHttpRequest();
            xhttp.onreadystatechange = function() {
              if (this.readyState == 4) {
                if (this.status == 200) {elmnt.innerHTML = this.responseText;}
                if (this.status == 404) {elmnt.innerHTML = "Page not found.";}
                /* Remove the attribute, and call this function once more: */
                elmnt.removeAttribute("w3-include-html");
                includeHTML();
              }
            }
            xhttp.open("GET", file, true);
            xhttp.send();
            /* Exit the function: */
            return;
          }
        }
      }

    </script> 

<script src="jquery.js"></script> 
<script> 
  $(function(){
    $("#attention_html").load("plots/attention.html")
  });
</script> 

  <script>
    includeHTML();
  </script> 
  </body>
</html>